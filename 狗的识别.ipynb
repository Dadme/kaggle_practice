{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "狗的识别.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dadme/kaggle_practice/blob/master/%E7%8B%97%E7%9A%84%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9B3a4wYUMBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import math\n",
        "from mxnet import autograd, gluon, init, nd\n",
        "from mxnet.gluon import data as gdata, loss as gloss, model_zoo, nn\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import zipfile\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJiwrvXOAnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSIHBQxYVFDI",
        "colab_type": "code",
        "outputId": "0feeb03a-818e-4e76-80b5-7f889f4933e6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2afd512-78fa-4402-b00d-f0a1846a335b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e2afd512-78fa-4402-b00d-f0a1846a335b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'utils.py': b'import collections\\nimport math\\nimport os\\nimport random\\nimport sys\\nimport tarfile\\nimport time\\nimport zipfile\\n\\nfrom IPython import display\\nfrom matplotlib import pyplot as plt\\nimport mxnet as mx\\nfrom mxnet import autograd, gluon, image, init, nd\\nfrom mxnet.contrib import text\\nfrom mxnet.gluon import data as gdata, loss as gloss, nn, utils as gutils\\nimport numpy as np\\n\\n\\nVOC_CLASSES = [\\'background\\', \\'aeroplane\\', \\'bicycle\\', \\'bird\\', \\'boat\\',\\n               \\'bottle\\', \\'bus\\', \\'car\\', \\'cat\\', \\'chair\\', \\'cow\\',\\n               \\'diningtable\\', \\'dog\\', \\'horse\\', \\'motorbike\\', \\'person\\',\\n               \\'potted plant\\', \\'sheep\\', \\'sofa\\', \\'train\\', \\'tv/monitor\\']\\n\\n\\nVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\\n                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\\n                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\\n                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\\n                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\\n                [0, 64, 128]]\\n\\n\\ndef bbox_to_rect(bbox, color):\\n    \"\"\"Convert bounding box to matplotlib format.\"\"\"\\n    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],\\n                         height=bbox[3]-bbox[1], fill=False, edgecolor=color,\\n                         linewidth=2)\\n\\n\\nclass Benchmark():\\n    \"\"\"Benchmark programs.\"\"\"\\n    def __init__(self, prefix=None):\\n        self.prefix = prefix + \\' \\' if prefix else \\'\\'\\n\\n    def __enter__(self):\\n        self.start = time.time()\\n\\n    def __exit__(self, *args):\\n        print(\\'%stime: %.4f sec\\' % (self.prefix, time.time() - self.start))\\n\\n\\ndef corr2d(X, K):\\n    \"\"\"Compute 2D cross-correlation.\"\"\"\\n    h, w = K.shape\\n    Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\\n    for i in range(Y.shape[0]):\\n        for j in range(Y.shape[1]):\\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\\n    return Y\\n\\n\\ndef count_tokens(samples):\\n    \"\"\"Count tokens in the data set.\"\"\"\\n    token_counter = collections.Counter()\\n    for sample in samples:\\n        for token in sample:\\n            if token not in token_counter:\\n                token_counter[token] = 1\\n            else:\\n                token_counter[token] += 1\\n    return token_counter\\n\\n\\ndef data_iter(batch_size, features, labels):\\n    \"\"\"Iterate through a data set.\"\"\"\\n    num_examples = len(features)\\n    indices = list(range(num_examples))\\n    random.shuffle(indices)\\n    for i in range(0, num_examples, batch_size):\\n        j = nd.array(indices[i: min(i + batch_size, num_examples)])\\n        yield features.take(j), labels.take(j)\\n\\n\\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\\n    \"\"\"Sample mini-batches in a consecutive order from sequential data.\"\"\"\\n    corpus_indices = nd.array(corpus_indices, ctx=ctx)\\n    data_len = len(corpus_indices)\\n    batch_len = data_len // batch_size\\n    indices = corpus_indices[0 : batch_size * batch_len].reshape((\\n        batch_size, batch_len))\\n    epoch_size = (batch_len - 1) // num_steps\\n    for i in range(epoch_size):\\n        i = i * num_steps\\n        X = indices[:, i : i + num_steps]\\n        Y = indices[:, i + 1 : i + num_steps + 1]\\n        yield X, Y\\n\\n\\ndef data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\\n    \"\"\"Sample mini-batches in a random order from sequential data.\"\"\"\\n    num_examples = (len(corpus_indices) - 1) // num_steps\\n    epoch_size = num_examples // batch_size\\n    example_indices = list(range(num_examples))\\n    random.shuffle(example_indices)\\n\\n    def _data(pos):\\n        return corpus_indices[pos : pos + num_steps]\\n\\n    for i in range(epoch_size):\\n        i = i * batch_size\\n        batch_indices = example_indices[i : i + batch_size]\\n        X = nd.array(\\n            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\\n        Y = nd.array(\\n            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\\n        yield X, Y\\n\\n\\ndef download_imdb(data_dir=\\'../data\\'):\\n    \"\"\"Download the IMDB data set for sentiment analysis.\"\"\"\\n    url = (\\'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\\')\\n    sha1 = \\'01ada507287d82875905620988597833ad4e0903\\'\\n    fname = gutils.download(url, data_dir, sha1_hash=sha1)\\n    with tarfile.open(fname, \\'r\\') as f:\\n        f.extractall(data_dir)\\n\\n\\ndef _download_pikachu(data_dir):\\n    root_url = (\\'https://apache-mxnet.s3-accelerate.amazonaws.com/\\'\\n                \\'gluon/dataset/pikachu/\\')\\n    dataset = {\\'train.rec\\': \\'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8\\',\\n               \\'train.idx\\': \\'dcf7318b2602c06428b9988470c731621716c393\\',\\n               \\'val.rec\\': \\'d6c33f799b4d058e82f2cb5bd9a976f69d72d520\\'}\\n    for k, v in dataset.items():\\n        gutils.download(root_url + k, os.path.join(data_dir, k), sha1_hash=v)\\n\\n\\ndef download_voc_pascal(data_dir=\\'../data\\'):\\n    \"\"\"Download the Pascal VOC2012 Dataset.\"\"\"\\n    voc_dir = os.path.join(data_dir, \\'VOCdevkit/VOC2012\\')\\n    url = (\\'http://host.robots.ox.ac.uk/pascal/VOC/voc2012\\'\\n           \\'/VOCtrainval_11-May-2012.tar\\')\\n    sha1 = \\'4e443f8a2eca6b1dac8a6c57641b67dd40621a49\\'\\n    fname = gutils.download(url, data_dir, sha1_hash=sha1)\\n    with tarfile.open(fname, \\'r\\') as f:\\n        f.extractall(data_dir)\\n    return voc_dir\\n\\n\\ndef evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\\n    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\\n    if isinstance(ctx, mx.Context):\\n        ctx = [ctx]\\n    acc_sum, n = nd.array([0]), 0\\n    for batch in data_iter:\\n        features, labels, _ = _get_batch(batch, ctx)\\n        for X, y in zip(features, labels):\\n            y = y.astype(\\'float32\\')\\n            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\\n            n += y.size\\n        acc_sum.wait_to_read()\\n    return acc_sum.asscalar() / n\\n\\n\\ndef _get_batch(batch, ctx):\\n    \"\"\"Return features and labels on ctx.\"\"\"\\n    features, labels = batch\\n    if labels.dtype != features.dtype:\\n        labels = labels.astype(features.dtype)\\n    return (gutils.split_and_load(features, ctx),\\n            gutils.split_and_load(labels, ctx), features.shape[0])\\n\\n\\ndef get_data_ch7():\\n    \"\"\"Get the data set used in Chapter 7.\"\"\"\\n    data = np.genfromtxt(\\'../data/airfoil_self_noise.dat\\', delimiter=\\'\\\\t\\')\\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\\n    return nd.array(data[:, :-1]), nd.array(data[:, -1])\\n\\n\\ndef get_fashion_mnist_labels(labels):\\n    \"\"\"Get text label for fashion mnist.\"\"\"\\n    text_labels = [\\'t-shirt\\', \\'trouser\\', \\'pullover\\', \\'dress\\', \\'coat\\',\\n                   \\'sandal\\', \\'shirt\\', \\'sneaker\\', \\'bag\\', \\'ankle boot\\']\\n    return [text_labels[int(i)] for i in labels]\\n\\n\\ndef get_tokenized_imdb(data):\\n    \"\"\"Get the tokenized IMDB data set for sentiment analysis.\"\"\"\\n    def tokenizer(text):\\n        return [tok.lower() for tok in text.split(\\' \\')]\\n    return [tokenizer(review) for review, _ in data]\\n\\n\\ndef get_vocab_imdb(data):\\n    \"\"\"Get the vocab for the IMDB data set for sentiment analysis.\"\"\"\\n    tokenized_data = get_tokenized_imdb(data)\\n    counter = collections.Counter([tk for st in tokenized_data for tk in st])\\n    return text.vocab.Vocabulary(counter, min_freq=5)\\n\\n\\ndef grad_clipping(params, theta, ctx):\\n    \"\"\"Clip the gradient.\"\"\"\\n    if theta is not None:\\n        norm = nd.array([0], ctx)\\n        for param in params:\\n            norm += (param.grad ** 2).sum()\\n        norm = norm.sqrt().asscalar()\\n        if norm > theta:\\n            for param in params:\\n                param.grad[:] *= theta / norm\\n\\n\\ndef linreg(X, w, b):\\n    \"\"\"Linear regression.\"\"\"\\n    return nd.dot(X, w) + b\\n\\n\\ndef load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\\n        \\'~\\', \\'.mxnet\\', \\'datasets\\', \\'fashion-mnist\\')):\\n    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\\n    root = os.path.expanduser(root)\\n    transformer = []\\n    if resize:\\n        transformer += [gdata.vision.transforms.Resize(resize)]\\n    transformer += [gdata.vision.transforms.ToTensor()]\\n    transformer = gdata.vision.transforms.Compose(transformer)\\n\\n    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\\n    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\\n    num_workers = 0 if sys.platform.startswith(\\'win32\\') else 4\\n\\n    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\\n                                  batch_size, shuffle=True,\\n                                  num_workers=num_workers)\\n    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\\n                                 batch_size, shuffle=False,\\n                                 num_workers=num_workers)\\n    return train_iter, test_iter\\n\\n\\ndef load_data_jay_lyrics():\\n    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\\n    with zipfile.ZipFile(\\'jaychou_lyrics.txt.zip\\') as zin:\\n        with zin.open(\\'jaychou_lyrics.txt\\') as f:\\n            corpus_chars = f.read().decode(\\'utf-8\\')\\n    corpus_chars = corpus_chars.replace(\\'\\\\n\\', \\' \\').replace(\\'\\\\r\\', \\' \\')\\n    corpus_chars = corpus_chars[0:10000]\\n    idx_to_char = list(set(corpus_chars))\\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\\n    vocab_size = len(char_to_idx)\\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size\\n\\n\\ndef load_data_pikachu(batch_size, edge_size=256):\\n    \"\"\"Download the pikachu dataest and then load into memory.\"\"\"\\n    data_dir = \\'../data/pikachu\\'\\n    _download_pikachu(data_dir)\\n    train_iter = image.ImageDetIter(\\n        path_imgrec=os.path.join(data_dir, \\'train.rec\\'),\\n        path_imgidx=os.path.join(data_dir, \\'train.idx\\'),\\n        batch_size=batch_size,\\n        data_shape=(3, edge_size, edge_size),\\n        shuffle=True,\\n        rand_crop=1,\\n        min_object_covered=0.95,\\n        max_attempts=200)\\n    val_iter = image.ImageDetIter(\\n        path_imgrec=os.path.join(data_dir, \\'val.rec\\'),\\n        batch_size=batch_size,\\n        data_shape=(3, edge_size, edge_size),\\n        shuffle=False)\\n    return train_iter, val_iter\\n\\n\\ndef load_data_time_machine():\\n    \"\"\"Load the time machine data set (available in the English book).\"\"\"\\n    with open(\\'../data/timemachine.txt\\') as f:\\n        corpus_chars = f.read()\\n    corpus_chars = corpus_chars.replace(\\'\\\\n\\', \\' \\').replace(\\'\\\\r\\', \\' \\').lower()\\n    corpus_chars = corpus_chars[0:10000]\\n    idx_to_char = list(set(corpus_chars))\\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\\n    vocab_size = len(char_to_idx)\\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size\\n\\n\\ndef _make_list(obj, default_values=None):\\n    if obj is None:\\n        obj = default_values\\n    elif not isinstance(obj, (list, tuple)):\\n        obj = [obj]\\n    return obj\\n\\n\\ndef mkdir_if_not_exist(path):\\n    \"\"\"Make a directory if it does not exist.\"\"\"\\n    if not os.path.exists(os.path.join(*path)):\\n        os.makedirs(os.path.join(*path))\\n\\n\\ndef predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\\n                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\\n    \"\"\"Predict next chars with a RNN model\"\"\"\\n    state = init_rnn_state(1, num_hiddens, ctx)\\n    output = [char_to_idx[prefix[0]]]\\n    for t in range(num_chars + len(prefix) - 1):\\n        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\\n        (Y, state) = rnn(X, state, params)\\n        if t < len(prefix) - 1:\\n            output.append(char_to_idx[prefix[t + 1]])\\n        else:\\n            output.append(int(Y[0].argmax(axis=1).asscalar()))\\n    return \\'\\'.join([idx_to_char[i] for i in output])\\n\\n\\ndef predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,\\n                      char_to_idx):\\n    \"\"\"Precit next chars with a Gluon RNN model\"\"\"\\n    state = model.begin_state(batch_size=1, ctx=ctx)\\n    output = [char_to_idx[prefix[0]]]\\n    for t in range(num_chars + len(prefix) - 1):\\n        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))\\n        (Y, state) = model(X, state)\\n        if t < len(prefix) - 1:\\n            output.append(char_to_idx[prefix[t + 1]])\\n        else:\\n            output.append(int(Y.argmax(axis=1).asscalar()))\\n    return \\'\\'.join([idx_to_char[i] for i in output])\\n\\n\\ndef predict_sentiment(net, vocab, sentence):\\n    \"\"\"Predict the sentiment of a given sentence.\"\"\"\\n    sentence = nd.array(vocab.to_indices(sentence), ctx=try_gpu())\\n    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\\n    return \\'positive\\' if label.asscalar() == 1 else \\'negative\\'\\n\\n\\ndef preprocess_imdb(data, vocab):\\n    \"\"\"Preprocess the IMDB data set for sentiment analysis.\"\"\"\\n    max_l = 500\\n\\n    def pad(x):\\n        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\\n\\n    tokenized_data = get_tokenized_imdb(data)\\n    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\\n    labels = nd.array([score for _, score in data])\\n    return features, labels\\n\\n\\ndef read_imdb(folder=\\'train\\'):\\n    \"\"\"Read the IMDB data set for sentiment analysis.\"\"\"\\n    data = []\\n    for label in [\\'pos\\', \\'neg\\']:\\n        folder_name = os.path.join(\\'../data/aclImdb/\\', folder, label)\\n        for file in os.listdir(folder_name):\\n            with open(os.path.join(folder_name, file), \\'rb\\') as f:\\n                review = f.read().decode(\\'utf-8\\').replace(\\'\\\\n\\', \\'\\').lower()\\n                data.append([review, 1 if label == \\'pos\\' else 0])\\n    random.shuffle(data)\\n    return data\\n\\n\\ndef read_voc_images(root=\\'../data/VOCdevkit/VOC2012\\', is_train=True):\\n    \"\"\"Read VOC images.\"\"\"\\n    txt_fname = \\'%s/ImageSets/Segmentation/%s\\' % (\\n        root, \\'train.txt\\' if is_train else \\'val.txt\\')\\n    with open(txt_fname, \\'r\\') as f:\\n        images = f.read().split()\\n    features, labels = [None] * len(images), [None] * len(images)\\n    for i, fname in enumerate(images):\\n        features[i] = image.imread(\\'%s/JPEGImages/%s.jpg\\' % (root, fname))\\n        labels[i] = image.imread(\\n            \\'%s/SegmentationClass/%s.png\\' % (root, fname))\\n    return features, labels\\n\\n\\nclass Residual(nn.Block):\\n    \"\"\"The residual block.\"\"\"\\n    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\\n        super(Residual, self).__init__(**kwargs)\\n        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\\n                               strides=strides)\\n        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\\n        if use_1x1conv:\\n            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\\n                                   strides=strides)\\n        else:\\n            self.conv3 = None\\n        self.bn1 = nn.BatchNorm()\\n        self.bn2 = nn.BatchNorm()\\n\\n    def forward(self, X):\\n        Y = nd.relu(self.bn1(self.conv1(X)))\\n        Y = self.bn2(self.conv2(Y))\\n        if self.conv3:\\n            X = self.conv3(X)\\n        return nd.relu(Y + X)\\n\\n\\ndef resnet18(num_classes):\\n    \"\"\"The ResNet-18 model.\"\"\"\\n    net = nn.Sequential()\\n    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\\n            nn.BatchNorm(), nn.Activation(\\'relu\\'))\\n\\n    def resnet_block(num_channels, num_residuals, first_block=False):\\n        blk = nn.Sequential()\\n        for i in range(num_residuals):\\n            if i == 0 and not first_block:\\n                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\\n            else:\\n                blk.add(Residual(num_channels))\\n        return blk\\n\\n    net.add(resnet_block(64, 2, first_block=True),\\n            resnet_block(128, 2),\\n            resnet_block(256, 2),\\n            resnet_block(512, 2))\\n    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\\n    return net\\n\\n\\nclass RNNModel(nn.Block):\\n    \"\"\"RNN model.\"\"\"\\n    def __init__(self, rnn_layer, vocab_size, **kwargs):\\n        super(RNNModel, self).__init__(**kwargs)\\n        self.rnn = rnn_layer\\n        self.vocab_size = vocab_size\\n        self.dense = nn.Dense(vocab_size)\\n\\n    def forward(self, inputs, state):\\n        X = nd.one_hot(inputs.T, self.vocab_size)\\n        Y, state = self.rnn(X, state)\\n        output = self.dense(Y.reshape((-1, Y.shape[-1])))\\n        return output, state\\n\\n    def begin_state(self, *args, **kwargs):\\n        return self.rnn.begin_state(*args, **kwargs)\\n\\n\\ndef semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\\n             legend=None, figsize=(3.5, 2.5)):\\n    \"\"\"Plot x and log(y).\"\"\"\\n    set_figsize(figsize)\\n    plt.xlabel(x_label)\\n    plt.ylabel(y_label)\\n    plt.semilogy(x_vals, y_vals)\\n    if x2_vals and y2_vals:\\n        plt.semilogy(x2_vals, y2_vals, linestyle=\\':\\')\\n        plt.legend(legend)\\n    plt.show()\\n\\n\\ndef set_figsize(figsize=(3.5, 2.5)):\\n    \"\"\"Set matplotlib figure size.\"\"\"\\n    use_svg_display()\\n    plt.rcParams[\\'figure.figsize\\'] = figsize\\n\\n\\ndef sgd(params, lr, batch_size):\\n    \"\"\"Mini-batch stochastic gradient descent.\"\"\"\\n    for param in params:\\n        param[:] = param - lr * param.grad / batch_size\\n\\n\\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\\n    \"\"\"Show bounding boxes.\"\"\"\\n    labels = _make_list(labels)\\n    colors = _make_list(colors, [\\'b\\', \\'g\\', \\'r\\', \\'m\\', \\'k\\'])\\n    for i, bbox in enumerate(bboxes):\\n        color = colors[i % len(colors)]\\n        rect = bbox_to_rect(bbox.asnumpy(), color)\\n        axes.add_patch(rect)\\n        if labels and len(labels) > i:\\n            text_color = \\'k\\' if color == \\'w\\' else \\'w\\'\\n            axes.text(rect.xy[0], rect.xy[1], labels[i],\\n                      va=\\'center\\', ha=\\'center\\', fontsize=9, color=text_color,\\n                      bbox=dict(facecolor=color, lw=0))\\n\\n\\ndef show_fashion_mnist(images, labels):\\n    \"\"\"Plot Fashion-MNIST images with labels.\"\"\"\\n    use_svg_display()\\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\\n    for f, img, lbl in zip(figs, images, labels):\\n        f.imshow(img.reshape((28, 28)).asnumpy())\\n        f.set_title(lbl)\\n        f.axes.get_xaxis().set_visible(False)\\n        f.axes.get_yaxis().set_visible(False)\\n\\n\\ndef show_images(imgs, num_rows, num_cols, scale=2):\\n    \"\"\"Plot a list of images.\"\"\"\\n    figsize = (num_cols * scale, num_rows * scale)\\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\\n    for i in range(num_rows):\\n        for j in range(num_cols):\\n            axes[i][j].imshow(imgs[i * num_cols + j].asnumpy())\\n            axes[i][j].axes.get_xaxis().set_visible(False)\\n            axes[i][j].axes.get_yaxis().set_visible(False)\\n    return axes\\n\\n\\ndef show_trace_2d(f, res):\\n    \"\"\"Show the trace of 2d variables during optimization.\"\"\"\\n    x1, x2 = zip(*res)\\n    set_figsize()\\n    plt.plot(x1, x2, \\'-o\\', color=\\'#ff7f0e\\')\\n    x1 = np.arange(-5.5, 1.0, 0.1)\\n    x2 = np.arange(min(-3.0, min(x2) - 1), max(1.0, max(x2) + 1), 0.1)\\n    x1, x2 = np.meshgrid(x1, x2)\\n    plt.contour(x1, x2, f(x1, x2), colors=\\'#1f77b4\\')\\n    plt.xlabel(\\'x1\\')\\n    plt.ylabel(\\'x2\\')\\n\\n\\ndef squared_loss(y_hat, y):\\n    \"\"\"Squared loss.\"\"\"\\n    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\\n\\n\\ndef to_onehot(X, size):\\n    \"\"\"Represent inputs with one-hot encoding.\"\"\"\\n    return [nd.one_hot(x, size) for x in X.T]\\n\\n\\ndef train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs):\\n    \"\"\"Train and evaluate a model.\"\"\"\\n    print(\\'training on\\', ctx)\\n    if isinstance(ctx, mx.Context):\\n        ctx = [ctx]\\n    for epoch in range(num_epochs):\\n        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\\n        for i, batch in enumerate(train_iter):\\n            Xs, ys, batch_size = _get_batch(batch, ctx)\\n            ls = []\\n            with autograd.record():\\n                y_hats = [net(X) for X in Xs]\\n                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\\n            for l in ls:\\n                l.backward()\\n            trainer.step(batch_size)\\n            train_l_sum += sum([l.sum().asscalar() for l in ls])\\n            n += sum([l.size for l in ls])\\n            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\\n                                 for y_hat, y in zip(y_hats, ys)])\\n            m += sum([y.size for y in ys])\\n        test_acc = evaluate_accuracy(test_iter, net, ctx)\\n        print(\\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, \\'\\n              \\'time %.1f sec\\'\\n              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\\n                 time.time() - start))\\n\\n\\ndef train_2d(trainer):\\n    \"\"\"Optimize the objective function of 2d variables with a customized trainer.\"\"\"\\n    x1, x2 = -5, -2\\n    s_x1, s_x2 = 0, 0\\n    res = [(x1, x2)]\\n    for i in range(20):\\n        x1, x2, s_x1, s_x2 = trainer(x1, x2, s_x1, s_x2)\\n        res.append((x1, x2))\\n    print(\\'epoch %d, x1 %f, x2 %f\\' % (i+1, x1, x2))\\n    return res\\n\\n\\ndef train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\\n                          vocab_size, ctx, corpus_indices, idx_to_char,\\n                          char_to_idx, is_random_iter, num_epochs, num_steps,\\n                          lr, clipping_theta, batch_size, pred_period,\\n                          pred_len, prefixes):\\n    \"\"\"Train an RNN model and predict the next item in the sequence.\"\"\"\\n    if is_random_iter:\\n        data_iter_fn = data_iter_random\\n    else:\\n        data_iter_fn = data_iter_consecutive\\n    params = get_params()\\n    loss = gloss.SoftmaxCrossEntropyLoss()\\n\\n    for epoch in range(num_epochs):\\n        if not is_random_iter:\\n            state = init_rnn_state(batch_size, num_hiddens, ctx)\\n        l_sum, n, start = 0.0, 0, time.time()\\n        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\\n        for X, Y in data_iter:\\n            if is_random_iter:\\n                state = init_rnn_state(batch_size, num_hiddens, ctx)\\n            else:\\n                for s in state:\\n                    s.detach()\\n            with autograd.record():\\n                inputs = to_onehot(X, vocab_size)\\n                (outputs, state) = rnn(inputs, state, params)\\n                outputs = nd.concat(*outputs, dim=0)\\n                y = Y.T.reshape((-1,))\\n                l = loss(outputs, y).mean()\\n            l.backward()\\n            grad_clipping(params, clipping_theta, ctx)\\n            sgd(params, lr, 1)\\n            l_sum += l.asscalar() * y.size\\n            n += y.size\\n\\n        if (epoch + 1) % pred_period == 0:\\n            print(\\'epoch %d, perplexity %f, time %.2f sec\\' % (\\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\\n            for prefix in prefixes:\\n                print(\\' -\\', predict_rnn(\\n                    prefix, pred_len, rnn, params, init_rnn_state,\\n                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))\\n\\n\\ndef train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\\n                                corpus_indices, idx_to_char, char_to_idx,\\n                                num_epochs, num_steps, lr, clipping_theta,\\n                                batch_size, pred_period, pred_len, prefixes):\\n    \"\"\"Train an Gluon RNN model and predict the next item in the sequence.\"\"\"\\n    loss = gloss.SoftmaxCrossEntropyLoss()\\n    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\\n    trainer = gluon.Trainer(model.collect_params(), \\'sgd\\',\\n                            {\\'learning_rate\\': lr, \\'momentum\\': 0, \\'wd\\': 0})\\n\\n    for epoch in range(num_epochs):\\n        l_sum, n, start = 0.0, 0, time.time()\\n        data_iter = data_iter_consecutive(\\n            corpus_indices, batch_size, num_steps, ctx)\\n        state = model.begin_state(batch_size=batch_size, ctx=ctx)\\n        for X, Y in data_iter:\\n            for s in state:\\n                s.detach()\\n            with autograd.record():\\n                (output, state) = model(X, state)\\n                y = Y.T.reshape((-1,))\\n                l = loss(output, y).mean()\\n            l.backward()\\n            params = [p.data() for p in model.collect_params().values()]\\n            grad_clipping(params, clipping_theta, ctx)\\n            trainer.step(1)\\n            l_sum += l.asscalar() * y.size\\n            n += y.size\\n\\n        if (epoch + 1) % pred_period == 0:\\n            print(\\'epoch %d, perplexity %f, time %.2f sec\\' % (\\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\\n            for prefix in prefixes:\\n                print(\\' -\\', predict_rnn_gluon(\\n                    prefix, pred_len, model, vocab_size, ctx, idx_to_char,\\n                    char_to_idx))\\n\\n\\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\\n              params=None, lr=None, trainer=None):\\n    \"\"\"Train and evaluate a model with CPU.\"\"\"\\n    for epoch in range(num_epochs):\\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\\n        for X, y in train_iter:\\n            with autograd.record():\\n                y_hat = net(X)\\n                l = loss(y_hat, y).sum()\\n            l.backward()\\n            if trainer is None:\\n                sgd(params, lr, batch_size)\\n            else:\\n                trainer.step(batch_size)\\n            y = y.astype(\\'float32\\')\\n            train_l_sum += l.asscalar()\\n            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\\n            n += y.size\\n        test_acc = evaluate_accuracy(test_iter, net)\\n        print(\\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f\\'\\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\\n\\n\\ndef train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\\n              num_epochs):\\n    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\\n    print(\\'training on\\', ctx)\\n    loss = gloss.SoftmaxCrossEntropyLoss()\\n    for epoch in range(num_epochs):\\n        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\\n        for X, y in train_iter:\\n            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\\n            with autograd.record():\\n                y_hat = net(X)\\n                l = loss(y_hat, y).sum()\\n            l.backward()\\n            trainer.step(batch_size)\\n            y = y.astype(\\'float32\\')\\n            train_l_sum += l.asscalar()\\n            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\\n            n += y.size\\n        test_acc = evaluate_accuracy(test_iter, net, ctx)\\n        print(\\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, \\'\\n              \\'time %.1f sec\\'\\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\\n                 time.time() - start))\\n\\n\\ndef train_ch7(trainer_fn, states, hyperparams, features, labels, batch_size=10,\\n              num_epochs=2):\\n    \"\"\"Train a linear regression model.\"\"\"\\n    net, loss = linreg, squared_loss\\n    w, b = nd.random.normal(scale=0.01, shape=(features.shape[1], 1)), nd.zeros(1)\\n    w.attach_grad()\\n    b.attach_grad()\\n\\n    def eval_loss():\\n        return loss(net(features, w, b), labels).mean().asscalar()\\n\\n    ls = [eval_loss()]\\n    data_iter = gdata.DataLoader(\\n        gdata.ArrayDataset(features, labels), batch_size, shuffle=True)\\n    for _ in range(num_epochs):\\n        start = time.time()\\n        for batch_i, (X, y) in enumerate(data_iter):\\n            with autograd.record():\\n                l = loss(net(X, w, b), y).mean()\\n            l.backward()\\n            trainer_fn([w, b], states, hyperparams)\\n            if (batch_i + 1) * batch_size % 100 == 0:\\n                ls.append(eval_loss())\\n    print(\\'loss: %f, %f sec per epoch\\' % (ls[-1], time.time() - start))\\n    set_figsize()\\n    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\\n    plt.xlabel(\\'epoch\\')\\n    plt.ylabel(\\'loss\\')\\n\\n\\ndef train_gluon_ch7(trainer_name, trainer_hyperparams, features, labels,\\n                    batch_size=10, num_epochs=2):\\n    \"\"\"Train a linear regression model with a given Gluon trainer.\"\"\"\\n    net = nn.Sequential()\\n    net.add(nn.Dense(1))\\n    net.initialize(init.Normal(sigma=0.01))\\n    loss = gloss.L2Loss()\\n\\n    def eval_loss():\\n        return loss(net(features), labels).mean().asscalar()\\n\\n    ls = [eval_loss()]\\n    data_iter = gdata.DataLoader(\\n        gdata.ArrayDataset(features, labels), batch_size, shuffle=True)\\n    trainer = gluon.Trainer(net.collect_params(),\\n                            trainer_name, trainer_hyperparams)\\n    for _ in range(num_epochs):\\n        start = time.time()\\n        for batch_i, (X, y) in enumerate(data_iter):\\n            with autograd.record():\\n                l = loss(net(X), y)\\n            l.backward()\\n            trainer.step(batch_size)\\n            if (batch_i + 1) * batch_size % 100 == 0:\\n                ls.append(eval_loss())\\n    print(\\'loss: %f, %f sec per epoch\\' % (ls[-1], time.time() - start))\\n    set_figsize()\\n    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\\n    plt.xlabel(\\'epoch\\')\\n    plt.ylabel(\\'loss\\')\\n\\n\\ndef try_all_gpus():\\n    \"\"\"Return all available GPUs, or [mx.cpu()] if there is no GPU.\"\"\"\\n    ctxes = []\\n    try:\\n        for i in range(16):\\n            ctx = mx.gpu(i)\\n            _ = nd.array([0], ctx=ctx)\\n            ctxes.append(ctx)\\n    except mx.base.MXNetError:\\n        pass\\n    if not ctxes:\\n        ctxes = [mx.cpu()]\\n    return ctxes\\n\\n\\ndef try_gpu():\\n    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\\n    try:\\n        ctx = mx.gpu()\\n        _ = nd.array([0], ctx=ctx)\\n    except mx.base.MXNetError:\\n        ctx = mx.cpu()\\n    return ctx\\n\\n\\ndef use_svg_display():\\n    \"\"\"Use svg format to display plot in jupyter\"\"\"\\n    display.set_matplotlib_formats(\\'svg\\')\\n\\n\\ndef voc_label_indices(colormap, colormap2label):\\n    \"\"\"Assign label indices for Pascal VOC2012 Dataset.\"\"\"\\n    colormap = colormap.astype(\\'int32\\')\\n    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\\n           + colormap[:, :, 2])\\n    return colormap2label[idx]\\n\\n\\ndef voc_rand_crop(feature, label, height, width):\\n    \"\"\"Random cropping for images of the Pascal VOC2012 Dataset.\"\"\"\\n    feature, rect = image.random_crop(feature, (width, height))\\n    label = image.fixed_crop(label, *rect)\\n    return feature, label\\n\\n\\nclass VOCSegDataset(gdata.Dataset):\\n    \"\"\"The Pascal VOC2012 Dataset.\"\"\"\\n    def __init__(self, is_train, crop_size, voc_dir, colormap2label):\\n        self.rgb_mean = nd.array([0.485, 0.456, 0.406])\\n        self.rgb_std = nd.array([0.229, 0.224, 0.225])\\n        self.crop_size = crop_size\\n        data, labels = read_voc_images(root=voc_dir, is_train=is_train)\\n        self.data = [self.normalize_image(im) for im in self.filter(data)]\\n        self.labels = self.filter(labels)\\n        self.colormap2label = colormap2label\\n        print(\\'read \\' + str(len(self.data)) + \\' examples\\')\\n\\n    def normalize_image(self, data):\\n        return (data.astype(\\'float32\\') / 255 - self.rgb_mean) / self.rgb_std\\n\\n    def filter(self, images):\\n        return [im for im in images if (\\n            im.shape[0] >= self.crop_size[0] and\\n            im.shape[1] >= self.crop_size[1])]\\n\\n    def __getitem__(self, idx):\\n        data, labels = voc_rand_crop(self.data[idx], self.labels[idx],\\n                                     *self.crop_size)\\n        return (data.transpose((2, 0, 1)),\\n                voc_label_indices(labels, self.colormap2label))\\n\\n    def __len__(self):\\n        return len(self.data)\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH-GEBOvVHkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo '{\"username\":\"shengleiwhut\",\"key\":\"5282ff6fee554360bfff9a7324afb7e4\"}' > /root/.kaggle/kaggle.json\n",
        "! chmod 600  /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYZZ54OVVNRy",
        "colab_type": "code",
        "outputId": "b7fed271-f06f-4c7b-f265-94d9baf6d6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!kaggle competitions download -c dog-breed-identification"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading labels.csv.zip to /content\n",
            "\r  0% 0.00/214k [00:00<?, ?B/s]\n",
            "\r100% 214k/214k [00:00<00:00, 80.8MB/s]\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Downloading test.zip to /content\n",
            " 97% 335M/346M [00:01<00:00, 228MB/s]\n",
            "100% 346M/346M [00:01<00:00, 199MB/s]\n",
            "Downloading train.zip to /content\n",
            " 98% 337M/345M [00:01<00:00, 201MB/s]\n",
            "100% 345M/345M [00:01<00:00, 184MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bvtMVuLVn1y",
        "colab_type": "code",
        "outputId": "f0e55bd0-2b1b-483a-be06-1ecc47c8a179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels.csv.zip\tsample_data\t\t   test.zip   utils.py\n",
            "__pycache__\tsample_submission.csv.zip  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1onbYO3PVtnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mv train.zip data\n",
        "!mv test.zip data\n",
        "!mv labels.csv.zip data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSOX9EXvWJVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXuCInjhV28a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'data'\n",
        "zipfiles = ['train.zip', 'test.zip', 'labels.csv.zip']\n",
        "for f in zipfiles:\n",
        "    with zipfile.ZipFile(data_dir + '/' + f, 'r') as z:\n",
        "        z.extractall(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1K7-KPVWAT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n",
        "    # 训练集中数量最少一类的狗的样本数\n",
        "    min_n_train_per_label = (\n",
        "        collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
        "    # 验证集中每类狗的样本数\n",
        "    n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n",
        "    label_count = {}\n",
        "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
        "        idx = train_file.split('.')[0]\n",
        "        label = idx_label[idx]\n",
        "        utils.mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
        "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
        "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
        "            utils.mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
        "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
        "            label_count[label] = label_count.get(label, 0) + 1\n",
        "        else:\n",
        "            utils.mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
        "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                        os.path.join(data_dir, input_dir, 'train', label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMd9zEOyWMqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
        "                   valid_ratio):\n",
        "    # 读取训练数据标签\n",
        "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
        "        lines = f.readlines()[1:]\n",
        "        tokens = [l.rstrip().split(',') for l in lines]\n",
        "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
        "    reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n",
        "\n",
        "    utils.mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
        "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
        "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
        "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPgDAllWQza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n",
        "input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
        "reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
        "                   valid_ratio)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvvQlZYiWU0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = gdata.vision.transforms.Compose([\n",
        "    gdata.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
        "                                              ratio=(3.0/4.0, 4.0/3.0)),\n",
        "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
        "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4,\n",
        "                                              saturation=0.4),\n",
        "    gdata.vision.transforms.RandomLighting(0.1),\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                      [0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFTalrjCWZ1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_test = gdata.vision.transforms.Compose([\n",
        "    gdata.vision.transforms.Resize(256),\n",
        "    gdata.vision.transforms.CenterCrop(224),\n",
        "    gdata.vision.transforms.ToTensor(),\n",
        "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                      [0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6-NB5mJWdVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
        "valid_ds = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(data_dir, input_dir, 'valid'), flag=1)\n",
        "train_valid_ds = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(data_dir, input_dir, 'train_valid'), flag=1)\n",
        "test_ds = gdata.vision.ImageFolderDataset(\n",
        "    os.path.join(data_dir, input_dir, 'test'), flag=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItoNV4QjWgWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
        "                              batch_size, shuffle=True, last_batch='keep')\n",
        "valid_iter = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
        "                              batch_size, shuffle=True, last_batch='keep')\n",
        "train_valid_iter = gdata.DataLoader(train_valid_ds.transform_first(\n",
        "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
        "test_iter = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
        "                             batch_size, shuffle=False, last_batch='keep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN00kmNAWjUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_net(ctx):\n",
        "    finetune_net = model_zoo.vision.resnet34_v2(pretrained=True)\n",
        "    # 定义输出网络\n",
        "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
        "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
        "    # 输出的类别120\n",
        "    finetune_net.output_new.add(nn.Dense(120))\n",
        "    # 初始化输出网络\n",
        "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
        "    finetune_net.collect_params().reset_ctx(ctx)\n",
        "    return finetune_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u8e4I6AWmbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "def evaluate_loss(data_iter, net, ctx):\n",
        "    l_sum, n = 0.0, 0\n",
        "    for X, y in data_iter:\n",
        "        y = y.as_in_context(ctx)\n",
        "        output_features = net.features(X.as_in_context(ctx))\n",
        "        outputs = net.output_new(output_features)\n",
        "        l_sum += loss(outputs, y).sum().asscalar()\n",
        "        n += y.size\n",
        "    return l_sum / n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ib-c4vWpfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
        "          lr_decay):\n",
        "    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n",
        "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, n, start = 0.0, 0, time.time()\n",
        "        if epoch > 0 and epoch % lr_period == 0:\n",
        "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
        "        for X, y in train_iter:\n",
        "            y = y.as_in_context(ctx)\n",
        "            output_features = net.features(X.as_in_context(ctx))\n",
        "            with autograd.record():\n",
        "                outputs = net.output_new(output_features)\n",
        "                l = loss(outputs, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            train_l_sum += l.asscalar()\n",
        "            n += y.size\n",
        "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
        "        if valid_iter is not None:\n",
        "            valid_loss = evaluate_loss(valid_iter, net, ctx)\n",
        "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, \"\n",
        "                       % (epoch + 1, train_l_sum / n, valid_loss))\n",
        "        else:\n",
        "            epoch_s = (\"epoch %d, train loss %f, \"\n",
        "                       % (epoch + 1, train_l_sum / n))\n",
        "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE47ydFlWttr",
        "colab_type": "code",
        "outputId": "c339e1a1-9978-45b6-e14e-b08576fc8751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "ctx, num_epochs, lr, wd = utils.try_gpu(), 10, 0.01, 1e-4\n",
        "lr_period, lr_decay, net = 10, 0.1, get_net(ctx)\n",
        "net.hybridize()\n",
        "train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
        "      lr_decay)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Mismatch in the content of model file detected. Downloading again.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/models/resnet34_v2-9d6b80bb.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet34_v2-9d6b80bb.zip...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mxnet/gluon/utils.py:331: UserWarning: File /root/.mxnet/models/resnet34_v2-9d6b80bb.zip exists in file system so the downloaded file is deleted\n",
            "  'File {} exists in file system so the downloaded file is deleted'.format(fname))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 3.300978, valid loss 1.258725, time 77.28 sec, lr 0.01\n",
            "epoch 2, train loss 1.280531, valid loss 0.690924, time 73.18 sec, lr 0.01\n",
            "epoch 3, train loss 1.037039, valid loss 0.566777, time 74.64 sec, lr 0.01\n",
            "epoch 4, train loss 0.962450, valid loss 0.538432, time 75.19 sec, lr 0.01\n",
            "epoch 5, train loss 0.874185, valid loss 0.498418, time 75.21 sec, lr 0.01\n",
            "epoch 6, train loss 0.841072, valid loss 0.485435, time 73.78 sec, lr 0.01\n",
            "epoch 7, train loss 0.831565, valid loss 0.478701, time 73.68 sec, lr 0.01\n",
            "epoch 8, train loss 0.811826, valid loss 0.445610, time 73.71 sec, lr 0.01\n",
            "epoch 9, train loss 0.798864, valid loss 0.456888, time 74.64 sec, lr 0.01\n",
            "epoch 10, train loss 0.764241, valid loss 0.440096, time 72.68 sec, lr 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RecB_NdWw6r",
        "colab_type": "code",
        "outputId": "dc603b90-f025-4bae-dd1e-96a4f9337dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "net = get_net(ctx)\n",
        "net.hybridize()\n",
        "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
        "      lr_decay)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 3.185491, time 81.29 sec, lr 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7az-pxxaY8t",
        "colab_type": "code",
        "outputId": "1a47c56e-6c29-4fe7-b72b-f58edeff3a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130811 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUc6D9bRds-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU5OaH3deQis",
        "colab_type": "code",
        "outputId": "2f44d610-431b-4f2d-ff64-a09d0ca858e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0JsOeboeUKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "for data, label in test_iter:\n",
        "    output_features = net.features(data.as_in_context(ctx))\n",
        "    output = nd.softmax(net.output_new(output_features))\n",
        "    preds.extend(output.asnumpy())\n",
        "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
        "with open('drive/submission.csv', 'w') as f:\n",
        "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
        "    for i, output in zip(ids, preds):\n",
        "        f.write(i.split('.')[0] + ',' + ','.join(\n",
        "            [str(num) for num in output]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqDLTBjJefqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}